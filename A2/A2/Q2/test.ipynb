{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import torch\r\n",
    "from torch.utils.data import DataLoader,WeightedRandomSampler\r\n",
    "\r\n",
    "weights = torch.FloatTensor([1,2,2,4,4,1])\r\n",
    "train_sampler = WeightedRandomSampler(weights,len(train_dataset),replacement=True)\r\n",
    "train_sampler = DataLoader(train_dataset,sampler=sampler)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-70d004384a04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_sampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWeightedRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtrain_sampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchtext.legacy.data import Field\r\n",
    "from torchtext import vocab\r\n",
    "import os\r\n",
    "import nltk\r\n",
    "corpus = [\"D'aww! He matches this background colour\",\r\n",
    "         \"Yo bitch Ja Rule is more succesful then\",\r\n",
    "         \"If you have a look back at the source\"]\r\n",
    "labels = [0,1,0]\r\n",
    "# 2.定义不同的Field\r\n",
    "TEXT = data.Field(sequential=True, lower=True, fix_length=10,tokenize=str.split,batch_first=True)\r\n",
    "LABEL = Field(sequential=False, use_vocab=False)\r\n",
    "fields = [(\"comment\", TEXT),(\"label\",LABEL)]\r\n",
    "# 3.将数据转换为Example对象的列表\r\n",
    "examples = []\r\n",
    "for text,label in zip(corpus,labels):\r\n",
    "    example = Example.fromlist([text,label],fields=fields)\r\n",
    "    examples.append(example)\r\n",
    "print(type(examples[0]))\r\n",
    "print(examples[0].comment)\r\n",
    "print(examples[0].label)\r\n",
    "# 4.构建词表\r\n",
    "new_corpus = [example.comment for example in examples]\r\n",
    "TEXT.build_vocab(new_corpus)\r\n",
    "print(TEXT.process(new_corpus))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import json\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import numpy as np\r\n",
    "from collections import Counter\r\n",
    "\r\n",
    "train_data = json.load(\r\n",
    "    open(\"genre_train.json\", \"r\"))\r\n",
    "\r\n",
    "X = train_data['X']\r\n",
    "Y = train_data['Y']\r\n",
    "docid = train_data['docid']\r\n",
    "# print(Counter(docid))\r\n",
    "print({0 : Counter(Y)[0]/len(Y), 1 : Counter(Y)[1]/len(Y),\r\n",
    "       2 : Counter(Y)[2]/len(Y), 3 : Counter(Y)[3]/len(Y)})\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.asarray(\r\n",
    "    X).reshape(-1, 1), np.asarray(Y).reshape(-1, 1), test_size=0.33, stratify=Y)\r\n",
    "X_train = []\r\n",
    "Y_train = []\r\n",
    "X_valid = []\r\n",
    "Y_valid = []\r\n",
    "X_test = []\r\n",
    "Y_test = []\r\n",
    "\r\n",
    "\r\n",
    "def train_valid_test(X, Y, docid, ratio=4):\r\n",
    "    X_train = []\r\n",
    "    Y_train = []\r\n",
    "    X_valid = []\r\n",
    "    Y_valid = []\r\n",
    "\r\n",
    "    idcount_dict = {}\r\n",
    "    valid_docid = []\r\n",
    "    for i, doc in enumerate(X):\r\n",
    "        doc_id = docid[i]\r\n",
    "        if doc_id in idcount_dict:\r\n",
    "            idcount_dict[doc_id] = idcount_dict[doc_id] + 1\r\n",
    "            num_doc              = idcount_dict[doc_id]\r\n",
    "            if (num_doc % ratio) == 0:\r\n",
    "                valid_docid.append(doc_id)\r\n",
    "                X_valid.append(doc)\r\n",
    "                Y_valid.append(Y[i])\r\n",
    "            else:\r\n",
    "                X_train.append(doc)\r\n",
    "                Y_train.append(Y[i])\r\n",
    "        else:\r\n",
    "            idcount_dict[doc_id] = 1\r\n",
    "            X_train.append(doc)\r\n",
    "            Y_train.append(Y[i])\r\n",
    "\r\n",
    "    return X_train, Y_train, X_valid, Y_valid, valid_docid\r\n",
    "\r\n",
    "X_train, Y_train, X_valid, Y_valid, valid_docid = train_valid_test(X,Y,docid,3)\r\n",
    "print(len(valid_docid) == len(X_valid))\r\n",
    "\r\n",
    "\r\n",
    "X_valid, Y_valid, X_test, Y_test,_ = train_valid_test(X_valid,Y_valid,valid_docid,2)\r\n",
    "print(len(Y_train))\r\n",
    "print(len(Y_valid))\r\n",
    "print(len(Y_test))\r\n",
    "\r\n",
    "print(len(Y_train)/6743)\r\n",
    "print(len(Y_valid)/6743)\r\n",
    "print(len(Y_test)/6743)\r\n",
    "\r\n",
    "print(Counter(Y_train))\r\n",
    "print(Counter(Y_valid))\r\n",
    "print(Counter(Y_test))\r\n",
    "\r\n",
    "print({0 : Counter(Y_train)[0]/len(Y_train), 1 : Counter(Y_train)[1]/len(Y_train),\r\n",
    "       2 : Counter(Y_train)[2]/len(Y_train), 3 : Counter(Y_train)[3]/len(Y_train)})\r\n",
    "print({0 : Counter(Y_valid)[0]/len(Y_valid), 1 : Counter(Y_valid)[1]/len(Y_valid),\r\n",
    "       2 : Counter(Y_valid)[2]/len(Y_valid), 3 : Counter(Y_valid)[3]/len(Y_valid)})\r\n",
    "print({0 : Counter(Y_test)[0]/len(Y_test), 1 : Counter(Y_test)[1]/len(Y_test),\r\n",
    "       2 : Counter(Y_test)[2]/len(Y_test), 3 : Counter(Y_test)[3]/len(Y_test)})\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0.049532848880320335, 1: 0.6962776212368382, 2: 0.19694497997923774, 3: 0.05724454990360374}\n",
      "True\n",
      "4765\n",
      "1142\n",
      "836\n",
      "0.7066587572297197\n",
      "0.169360818626724\n",
      "0.12398042414355628\n",
      "Counter({1: 3355, 2: 916, 3: 265, 0: 229})\n",
      "Counter({1: 793, 2: 226, 3: 65, 0: 58})\n",
      "Counter({1: 547, 2: 186, 3: 56, 0: 47})\n",
      "{0: 0.04805876180482686, 1: 0.7040923399790137, 2: 0.19223504721930745, 3: 0.055613850996852045}\n",
      "{0: 0.050788091068301226, 1: 0.6943957968476357, 2: 0.1978984238178634, 3: 0.05691768826619965}\n",
      "{0: 0.056220095693779906, 1: 0.6543062200956937, 2: 0.22248803827751196, 3: 0.06698564593301436}\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "813112ed64e20f51b539095a608bda2b54bfc9bcb8cdad5fb223e90931f4f9a3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}